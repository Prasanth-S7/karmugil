{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca4da5ba-888b-49bd-aeec-2f3ecb7343d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flood Occurrence Probability: 99.60%\n",
      "Flood occurrence probability has been updated in the database.\n"
     ]
    }
   ],
   "source": [
    "import schedule\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "def job():\n",
    "# Connect to MongoDB\n",
    "    client = MongoClient(\"mongodb+srv://sai:sai123Sai@cluster0.qcpd6.mongodb.net/\")  # Replace with your MongoDB connection string\n",
    "    db = client['Soonu']  # Replace with your database name\n",
    "    collection = db['NunDat']  # Replace with your collection name\n",
    "\n",
    "    # Read the CSV file for training\n",
    "    df = pd.read_csv(\"C:/Users/links/OneDrive/Desktop/convert1.csv\")\n",
    "\n",
    "    # Handle missing values\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())\n",
    "\n",
    "    # Define features and target\n",
    "    feature_columns = ['HOUR1', 'HOUR2', 'HOUR3', 'HOUR4', 'HOUR5', 'HOUR6', 'HOUR7', 'HOUR8',\n",
    "                    'HOUR9', 'HOUR10', 'HOUR11', 'HOUR12', 'HOUR13', 'HOUR14', 'HOUR15',\n",
    "                    'HOUR16', 'HOUR17', 'HOUR18', 'HOUR19', 'HOUR20', 'HOUR21', 'HOUR22', \n",
    "                    'HOUR23', 'HOUR24']\n",
    "\n",
    "    X = df[feature_columns]  \n",
    "    y = df['Flood_Occurence']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Impute and scale the data\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_imputed = imputer.fit_transform(X_train)\n",
    "    X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "    # Feature selection using RandomForestClassifier\n",
    "    selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold='mean')\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "    X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "    # Initialize base models\n",
    "    rf = RandomForestClassifier(random_state=42, n_estimators=200, max_depth=15, min_samples_split=10, min_samples_leaf=5)\n",
    "    gbc = GradientBoostingClassifier(random_state=42, n_estimators=150, learning_rate=0.05, max_depth=4, min_samples_split=10, min_samples_leaf=5, subsample=0.8)\n",
    "\n",
    "    # Combine them using a Voting Classifier\n",
    "    voting_clf = VotingClassifier(estimators=[('rf', rf), ('gbc', gbc)], voting='soft')\n",
    "\n",
    "    # Fit the ensemble model\n",
    "    voting_clf.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Retrieve the latest document from MongoDB\n",
    "    document = collection.find_one(sort=[('_id', -1)])  # Get the latest document\n",
    "\n",
    "    # Extract the 24-hour data from the document\n",
    "    test = pd.DataFrame([{\n",
    "        'HOUR1': document['hr 1'],\n",
    "        'HOUR2': document['hr 2'],\n",
    "        'HOUR3': document['hr 3'],\n",
    "        'HOUR4': document['hr 4'],\n",
    "        'HOUR5': document['hr 5'],\n",
    "        'HOUR6': document['hr 6'],\n",
    "        'HOUR7': document['hr 7'],\n",
    "        'HOUR8': document['hr 8'],\n",
    "        'HOUR9': document['hr 9'],\n",
    "        'HOUR10': document['hr 10'],\n",
    "        'HOUR11': document['hr 11'],\n",
    "        'HOUR12': document['hr 12'],\n",
    "        'HOUR13': document['hr 13'],\n",
    "        'HOUR14': document['hr 14'],\n",
    "        'HOUR15': document['hr 15'],\n",
    "        'HOUR16': document['hr 16'],\n",
    "        'HOUR17': document['hr 17'],\n",
    "        'HOUR18': document['hr 18'],\n",
    "        'HOUR19': document['hr 19'],\n",
    "        'HOUR20': document['hr 20'],\n",
    "        'HOUR21': document['hr 21'],\n",
    "        'HOUR22': document['hr 22'],\n",
    "        'HOUR23': document['hr 23'],\n",
    "        'HOUR24': document['hr 24']\n",
    "    }])\n",
    "\n",
    "    # Impute and scale the new input\n",
    "    test_imputed = imputer.transform(test)\n",
    "    test_scaled = scaler.transform(test_imputed)\n",
    "    test_selected = selector.transform(test_scaled)\n",
    "\n",
    "    # Predict using the trained model\n",
    "    probabilities = voting_clf.predict_proba(test_selected)  # Get the probabilities for each class\n",
    "\n",
    "    # Probability of flood occurrence (class 1)\n",
    "    flood_probability = probabilities[0][1] * 100\n",
    "\n",
    "    print(f\"Flood Occurrence Probability: {flood_probability:.2f}%\")\n",
    "    collection.update_one(\n",
    "        {'_id': document['_id']},  # Filter to match the latest document\n",
    "        {'$set': {'pe': flood_probability}}\n",
    "    )\n",
    "\n",
    "    print(\"Flood occurrence probability has been updated in the database.\")\n",
    "schedule.every(15).seconds.do(job)\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f78fa-1e02-4253-9456-71079e18b59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788e829-eda7-4683-bece-1f81ee555471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad6ed5-a49b-4c9c-8d20-095762a96cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
